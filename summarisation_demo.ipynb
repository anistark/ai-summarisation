{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AI Summarisation Tool\n\nQuickly summarize long articles, research papers, and documents using AI. Then evaluate how good your summaries are with objective metrics so you can improve them.\n\n**What we'll do:**\n1. Load an article or document\n2. Generate a summary using an LLM\n3. Check how good it is using evaluation metrics\n4. Get insights on how to improve\n\nLet's get started! üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to install (first time only)\n# !pip install ragas langchain-openai pypdf python-docx\n\nimport os\nimport asyncio\nfrom typing import Optional\n\n# Ragas imports\nfrom ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import SummarizationScore, Faithfulness, DiscreteMetric\nfrom ragas.llms import llm_factory\n\nprint(\"‚úÖ Imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Configuration from .env\n\nFirst, let's load your API keys from the .env file (keep them out of code!)\n\nTo get started:\n1. Copy `.env.example` to `.env`\n2. Add your API key to `.env`\n3. Run the cell below"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get API key and model from .env\napi_key = os.environ.get(\"OPENAI_API_KEY\")\nllm_model = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")\n\n# Verify API key is set\nif not api_key:\n    print(\"‚ö†Ô∏è  OPENAI_API_KEY not found in .env\")\n    print(\"\\nTo fix this:\")\n    print(\"1. Copy .env.example to .env\")\n    print(\"2. Add your OpenAI API key: OPENAI_API_KEY=sk-...\")\n    print(\"3. Re-run this cell\")\nelse:\n    # Initialize the LLM\n    try:\n        llm = llm_factory(llm_model)\n        print(\"‚úÖ LLM configured successfully\")\n        print(f\"   Model: {llm_model}\")\n        print(f\"   Provider: {llm.__class__.__name__}\")\n    except Exception as e:\n        print(f\"‚ùå Error initializing LLM: {e}\")\n        print(f\"\\nMake sure your API key is valid and set in .env\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Example Article\n",
    "\n",
    "We'll use built-in examples first. Later, modify this to load your own files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Apple AI Investment\n",
    "article_1 = \"\"\"\n",
    "Apple announced on Tuesday that it will invest $1 billion in new AI research centers\n",
    "across the United States over the next five years. The company plans to hire 500 new\n",
    "researchers and engineers specifically for AI development. CEO Tim Cook stated that\n",
    "artificial intelligence is central to the company's future product strategy. The investment\n",
    "will focus on areas like natural language processing, computer vision, and machine learning\n",
    "efficiency. Apple will establish research hubs in San Francisco, Boston, and Seattle.\n",
    "The company already employs over 10,000 AI researchers globally.\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Electric Vehicles & Environment\n",
    "article_2 = \"\"\"\n",
    "A recent study by Stanford researchers found that electric vehicles (EVs) can reduce\n",
    "carbon emissions by 70% compared to gasoline cars over their lifetime, when considering\n",
    "manufacturing and electricity sources. However, this varies significantly by region based\n",
    "on how electricity is generated. In regions with renewable energy sources, the reduction\n",
    "can reach 90%, while in coal-dependent regions it drops to 40%. The study analyzed over\n",
    "200 million vehicle registrations across 60 countries. Researchers emphasize that as power\n",
    "grids become cleaner, the environmental benefit of EVs will only improve.\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Medical Treatment Approval\n",
    "article_3 = \"\"\"\n",
    "The FDA approved a new diabetes treatment on Wednesday that requires injections only once\n",
    "per week instead of daily. Clinical trials showed 85% of patients achieved their target\n",
    "blood sugar levels within 3 months. The drug, developed by Novo Nordisk, is expected to\n",
    "reduce patient burden and improve medication adherence. Side effects were minimal and similar\n",
    "to existing treatments. The drug will be priced at $400 per month, which pharmaceutical\n",
    "experts say is competitive with existing weekly injection alternatives.\n",
    "\"\"\"\n",
    "\n",
    "# Select which article to use\n",
    "selected_article = article_1  # Change to article_2 or article_3 to try others\n",
    "\n",
    "print(\"üì∞ Selected Article:\")\n",
    "print(\"=\"*70)\n",
    "print(selected_article.strip())\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Summary\n",
    "\n",
    "Create a summary using an LLM (or provide your own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Initialize LLM for summarization\nsummarizer_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n\n# Create summarization prompt\nsummarization_prompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"\"\"Summarize the following text in 2-3 sentences. Focus on the main points and key details.\n\nText:\n{text}\n\nSummary:\"\"\"\n)\n\n# Generate summary\nchain = summarization_prompt | summarizer_llm\nsummary_response = chain.invoke({\"text\": selected_article})\ngenerated_summary = summary_response.content.strip()\n\nprint(\"‚úçÔ∏è  Generated Summary:\")\nprint(\"=\"*70)\nprint(generated_summary)\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Prepare for Evaluation\n\nFor the new metrics, we need to format our data properly. SummarizationScore expects reference contexts (the original text)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create evaluation sample with proper format\n# SummarizationScore needs reference_contexts (the original text split into chunks)\nsample = SingleTurnSample(\n    user_input=\"\",  # Not used for summarization metrics\n    response=generated_summary,\n    reference_contexts=[selected_article.strip()],  # Original article as context\n)\n\n# Also create a sample for Faithfulness metric\nfaithfulness_sample = SingleTurnSample(\n    user_input=selected_article.strip(),\n    response=generated_summary,\n)\n\nprint(\"‚úÖ Evaluation samples prepared\")\nprint(f\"   Article length: {len(selected_article.split())} words\")\nprint(f\"   Summary length: {len(generated_summary.split())} words\")\nprint(f\"   Compression ratio: {len(selected_article.split()) / len(generated_summary.split()):.1f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Evaluation Sample\n",
    "\n",
    "Format data for Ragas evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ragas sample\n",
    "sample = SingleTurnSample(\n",
    "    user_input=selected_article.strip(),\n",
    "    response=generated_summary,\n",
    "    reference=reference_summary.strip()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample created for evaluation\")\n",
    "print(f\"   Article length: {len(selected_article.split())} words\")\n",
    "print(f\"   Summary length: {len(generated_summary.split())} words\")\n",
    "print(f\"   Compression ratio: {len(selected_article.split()) / len(generated_summary.split()):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Evaluate Summary Quality\n\nNow let's evaluate how good the summary is using three metrics. These metrics will help us understand if we should trust the summary or improve it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the new metrics with the LLM\nsummarization_metric = SummarizationScore(llm=llm)\nfaithfulness_metric = Faithfulness(llm=llm)\n\n# Create a discrete metric for overall quality assessment\nquality_metric = DiscreteMetric(\n    name=\"summary_quality\",\n    criteria=\"Evaluate if the summary is accurate, complete, and well-written\",\n    allowed_values=[\"poor\", \"fair\", \"good\", \"excellent\"],\n    llm=llm,\n)\n\nprint(\"‚úÖ Evaluation metrics initialized\")\nprint(\"\\nüìä We'll measure:\")\nprint(\"   1. Summarization Score - QA-based + conciseness\")\nprint(\"   2. Faithfulness - Is summary grounded in source?\")\nprint(\"   3. Overall Quality - Categorical assessment (poor/fair/good/excellent)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Run Quality Check üîç\n\nThis will evaluate the summary. Depending on your LLM, this takes 30-60 seconds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def evaluate_summary():\n    \"\"\"Run all evaluation metrics\"\"\"\n    print(\"üîç Evaluating summary...\\n\")\n    \n    try:\n        # 1. Summarization Score (QA-based + conciseness)\n        print(\"‚è≥ Computing Summarization Score...\")\n        summ_score = await summarization_metric.ascore(sample)\n        print(f\"   ‚úÖ Summarization Score: {summ_score:.3f}\\n\")\n        \n        # 2. Faithfulness (is it grounded in source?)\n        print(\"‚è≥ Computing Faithfulness...\")\n        faith_score = await faithfulness_metric.ascore(faithfulness_sample)\n        print(f\"   ‚úÖ Faithfulness: {faith_score:.3f}\\n\")\n        \n        # 3. Overall Quality (discrete: poor/fair/good/excellent)\n        print(\"‚è≥ Computing Overall Quality...\")\n        quality_result = await quality_metric.ascore(faithfulness_sample)\n        print(f\"   ‚úÖ Overall Quality: {quality_result}\\n\")\n        \n        return {\n            \"summarization_score\": summ_score,\n            \"faithfulness\": faith_score,\n            \"overall_quality\": quality_result\n        }\n    \n    except Exception as e:\n        print(f\"‚ùå Error during evaluation: {e}\")\n        print(\"\\nüí° Tip: Make sure your LLM API key is set correctly!\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Run evaluation\nresults = await evaluate_summary()\n\nif results:\n    print(\"=\"*70)\n    print(\"EVALUATION COMPLETE!\")\n    print(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Review Results üìä\n\nLet's see how good your summary is and what to improve."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if results:\n    summ_score = results[\"summarization_score\"]\n    faith = results[\"faithfulness\"]\n    quality = results[\"overall_quality\"]\n    \n    # Display scores\n    print(\"\\nüìä EVALUATION RESULTS:\\n\")\n    \n    print(f\"1Ô∏è‚É£  SUMMARIZATION SCORE: {summ_score:.3f}/1.0\")\n    print(f\"   Measures how well the summary captures key information\")\n    print(f\"   (Based on QA + conciseness)\")\n    if summ_score >= 0.8:\n        print(\"   ‚úÖ Excellent - Summary captures content well\")\n    elif summ_score >= 0.6:\n        print(\"   ‚úÖ Good - Summary covers main points\")\n    elif summ_score >= 0.4:\n        print(\"   ‚ö†Ô∏è  Fair - Summary could be improved\")\n    else:\n        print(\"   ‚ùå Poor - Summary is missing important content\")\n    \n    print(f\"\\n2Ô∏è‚É£  FAITHFULNESS: {faith:.3f}/1.0\")\n    print(f\"   Is the summary factually grounded in the source?\")\n    if faith >= 0.9:\n        print(\"   ‚úÖ Excellent - Summary sticks to source material\")\n    elif faith >= 0.7:\n        print(\"   ‚úÖ Good - Mostly accurate\")\n    elif faith >= 0.5:\n        print(\"   ‚ö†Ô∏è  Fair - Some ungrounded claims\")\n    else:\n        print(\"   ‚ùå Poor - Contains hallucinations\")\n    \n    print(f\"\\n3Ô∏è‚É£  OVERALL QUALITY: {quality}\")\n    print(f\"   Expert assessment of summary quality\")\n    quality_color = {\n        \"excellent\": \"‚úÖ\",\n        \"good\": \"‚úÖ\",\n        \"fair\": \"‚ö†Ô∏è\",\n        \"poor\": \"‚ùå\"\n    }\n    print(f\"   {quality_color.get(quality, '‚ùì')} {quality.upper()}\")\n    \n    # Overall verdict\n    print(\"\\n\" + \"=\"*70)\n    print(\"VERDICT:\")\n    print(\"=\"*70)\n    \n    if summ_score >= 0.8 and faith >= 0.8 and quality in [\"good\", \"excellent\"]:\n        print(f\"‚úÖ HIGH QUALITY SUMMARY\")\n        print(\"\\n   This summary is accurate, complete, and trustworthy.\")\n        print(\"   Safe to publish or share with confidence.\")\n    elif summ_score >= 0.6 and faith >= 0.7 and quality in [\"fair\", \"good\", \"excellent\"]:\n        print(f\"‚úÖ GOOD QUALITY SUMMARY\")\n        print(\"\\n   Summary is generally accurate and captures key points.\")\n        print(\"   Minor review recommended before publishing.\")\n    elif summ_score >= 0.4 and faith >= 0.5:\n        print(f\"‚ö†Ô∏è  ACCEPTABLE SUMMARY\")\n        print(\"\\n   Summary has merit but needs improvement.\")\n        print(\"   Consider revising the summarization prompt or trying a different model.\")\n    else:\n        print(f\"‚ùå POOR QUALITY SUMMARY\")\n        print(\"\\n   Summary has significant issues.\")\n        print(\"   Recommend regenerating with different approaches.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Try With Your Own Content\n\nNow you can load your own articles and evaluate them. Modify the code below to load your files."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from plain text file\n",
    "# with open(\"/path/to/your/article.txt\", \"r\") as f:\n",
    "#     your_article = f.read()\n",
    "\n",
    "# Option 2: Load from PDF (requires pypdf)\n",
    "# from pypdf import PdfReader\n",
    "# reader = PdfReader(\"/path/to/your/document.pdf\")\n",
    "# your_article = \"\".join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "# Option 3: Load from Word document (requires python-docx)\n",
    "# from docx import Document\n",
    "# doc = Document(\"/path/to/your/document.docx\")\n",
    "# your_article = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "# Option 4: Just paste your text directly\n",
    "your_article = \"\"\"\n",
    "Paste your article here...\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Ready to evaluate your own content!\")\n",
    "print(f\"\\nArticle loaded ({len(your_article.split())} words)\")\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"   1. Edit the 'selected_article' variable (Step 3) to use your_article\")\n",
    "print(\"   2. Update the reference summary (Step 5)\")\n",
    "print(\"   3. Run Steps 4-9 again to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö Understanding the Evaluation Metrics\n\nThese metrics help you understand if your summary is good and how to improve it.\n\n### What Faithfulness Tells You\n- **Definition:** Is the summary based on the source text, or does it hallucinate?\n- **Why care:** A summary that makes up facts is worse than useless\n- **Low score means:** The LLM added information not in the original text\n- **How to improve:** Use a different model or stricter summarization prompt\n\n### What Answer Relevance Tells You  \n- **Definition:** Does the summary capture the important points?\n- **Why care:** A summary that's accurate but missing key info defeats the purpose\n- **Low score means:** Important information was left out\n- **How to improve:** Allow longer summaries or focus the prompt on key topics\n\n### What ROUGE Score Tells You\n- **Definition:** How similar is your summary to a reference summary?\n- **Why care:** Professional summaries tend to follow similar patterns\n- **Low score means:** Your summary took a different approach (not necessarily bad)\n- **How to improve:** Compare with reference and adjust your prompt\n\n---\n\n## üöÄ Tips for Better Summaries\n\n1. **Experiment with prompts** - Try \"Summarize in 3 bullet points\" vs \"Summarize in 1 paragraph\"\n2. **Try different models** - GPT-4 may score higher than GPT-3.5\n3. **Adjust length** - Longer summaries capture more details (but less compression)\n4. **Compare results** - Run the same article multiple times to see consistency\n5. **Use as feedback loop** - Use the metrics to refine your summarization strategy\n\nHappy summarizing! üéâ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python/x-name+python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}